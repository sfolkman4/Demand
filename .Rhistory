rf_workflow <- workflow() |>
add_model(rf_model) |>
add_recipe(my_recipe)
# Create cross-validation folds
storeItem_folds <- vfold_cv(storeItem, v = 5)
library(tidymodels)
# Create cross-validation folds
storeItem_folds <- vfold_cv(storeItem, v = 5)
# Define tuning grid
rf_grid <- grid_regular(
trees(range = c(100, 1000)),
min_n(range = c(2, 20)),
levels = 5
)
# Tune workflow
rf_tune <- rf_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = rf_grid,
metrics = metric_set(smape)
)
storeItem <- train |>
filter(store == 4, item == 10)
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month")) %>%
step_rm(date)
rf_model <- rand_forest(
trees = tune(),       # Tune number of trees
min_n = tune()        # Tune minimum node size
) %>%
set_mode("regression") %>%
set_engine("ranger")
# Create workflow
rf_workflow <- workflow() |>
add_model(rf_model) |>
add_recipe(my_recipe)
# Create cross-validation folds
storeItem_folds <- vfold_cv(storeItem, v = 5)
# Define tuning grid
rf_grid <- grid_regular(
trees(range = c(100, 1000)),
min_n(range = c(2, 20)),
levels = 5
)
# Tune workflow
rf_tune <- rf_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = rf_grid,
metrics = metric_set(smape)
)
# Select best model
best_rf <- rf_tune %>%
show_best(metric = "smape")
best_rf
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month", "doy")) %>%
step_rm(date)
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month", "doy")) %>%
step_range(date_doy, min=0, max=pi) %>%
step_mutate(sinDOY=sin(date_doy), cosDOY=cos(date_doy)) |>
step_rm(date)
rf_model <- rand_forest(
trees = tune(),
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
rf_workflow <- workflow() |>
add_model(rf_model) |>
add_recipe(my_recipe)
storeItem_folds <- vfold_cv(storeItem, v = 5)
rf_grid <- grid_regular(
trees(range = c(100, 1000)),
min_n(range = c(2, 20)),
levels = 5
)
rf_tune <- rf_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = rf_grid,
metrics = metric_set(smape)
)
best_rf <- rf_tune %>%
show_best(metric = "smape")
best_rf
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month", "doy")) %>%
step_range(date_doy, min=0, max=pi) %>%
step_mutate(sinDOY=sin(date_doy), cosDOY=cos(date_doy)) |>
step_mutate(year_numeric = as.numeric(date_year))
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month", "doy")) %>%
step_range(date_doy, min=0, max=pi) %>%
step_mutate(sinDOY=sin(date_doy), cosDOY=cos(date_doy)) |>
step_mutate(year_numeric = as.numeric(date_year)) |>
step_rm(date)
rf_model <- rand_forest(
trees = tune(),
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
rf_workflow <- workflow() |>
add_model(rf_model) |>
add_recipe(my_recipe)
storeItem_folds <- vfold_cv(storeItem, v = 5)
rf_grid <- grid_regular(
trees(range = c(100, 1000)),
min_n(range = c(2, 20)),
levels = 5
)
rf_tune <- rf_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = rf_grid,
metrics = metric_set(smape)
)
my_recipe <- recipe(sales ~ ., data = storeItem) %>%
step_date(date, features = c("dow", "month", "doy", "year")) %>%
step_range(date_doy, min=0, max=pi) %>%
step_mutate(sinDOY=sin(date_doy), cosDOY=cos(date_doy)) |>
step_mutate(year_numeric = as.numeric(date_year)) |>
step_rm(date)
rf_model <- rand_forest(
trees = tune(),
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
rf_workflow <- workflow() |>
add_model(rf_model) |>
add_recipe(my_recipe)
storeItem_folds <- vfold_cv(storeItem, v = 5)
rf_grid <- grid_regular(
trees(range = c(100, 1000)),
min_n(range = c(2, 20)),
levels = 5
)
rf_tune <- rf_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = rf_grid,
metrics = metric_set(smape)
)
best_rf <- rf_tune %>%
show_best(metric = "smape")
best_rf
# Penalized linear regression model
en_model <- linear_reg(
penalty = tune(),     # L1/L2 penalty
mixture = tune()      # Mixture of L1 and L2 penalties
) %>%
set_engine("glmnet")
# Workflow
en_workflow <- workflow() %>%
add_model(en_model) %>%
add_recipe(my_recipe)
# Tuning grid
en_grid <- grid_regular(
penalty(range = c(-10, 0)),   # log-transformed penalty
mixture(range = c(0, 1)),     # mixture of L1/L2
levels = 5
)
# Tune model
en_tune <- en_workflow %>%
tune_grid(
resamples = storeItem_folds,
grid = en_grid,
metrics = metric_set(smape)
)
best_rf
storeItem %>%
ggplot(mapping=aes(x=day, y=sales)) + geom_line() + geom_smooth(se=FALSE)
View(storeItem)
storeItem %>%
ggplot(mapping=aes(x=date, y=sales)) + geom_line() + geom_smooth(se=FALSE)
View(storeItem)
storeItem %>%
ggplot(mapping=aes(x=date, y=sales)) + geom_line() + geom_smooth(se=FALSE)
storeItem |>
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE)
storeItem |>
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE)
p <- storeItem |>
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE)
ggsave("sales_plot.png", p)
p <- storeItem |>
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE)
ggsave("sales_plot.png", p)
ggsave("C:/Users/sfolk/Downloads/sales_plot.png", p)
storeItem <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
library(forecast)
library(gridExtra)
test <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/test.csv")
storeItem <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
# Time series plots
p1 <- storeItem %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
p2 <- storeItem2 %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
# ACF plots using ggAcf
p3 <- storeItem %>%
pull(sales) %>%
ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
# Time series plots
p1 <- storeItem %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
p2 <- storeItem2 %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
# ACF plots using forecast::ggAcf
p3 <- storeItem %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
install.packages("forecast")
storeItem <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
# Time series plots
p1 <- storeItem %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
p2 <- storeItem2 %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
# ACF plots using forecast::ggAcf
p3 <- storeItem %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
p4 <- storeItem2 %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
p5 <- storeItem %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 2*365) +
theme_minimal() +
theme(axis.title = element_blank())
p6 <- storeItem2 %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 2*365) +
theme_minimal() +
theme(axis.title = element_blank())
library(forecast)
install.packages("forecast")
library(forecast)
train <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/train.csv")
test <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/test.csv")
storeItem <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
# Time series plots
p1 <- storeItem %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
p2 <- storeItem2 %>%
ggplot(aes(x = date, y = sales)) +
geom_line() +
geom_smooth(se = FALSE) +
theme_minimal() +
theme(axis.title = element_blank())
# ACF plots using forecast::ggAcf
p3 <- storeItem %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
p4 <- storeItem2 %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 30) +
theme_minimal() +
theme(axis.title = element_blank())
p5 <- storeItem %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 2*365) +
theme_minimal() +
theme(axis.title = element_blank())
p6 <- storeItem2 %>%
pull(sales) %>%
forecast::ggAcf(lag.max = 2*365) +
theme_minimal() +
theme(axis.title = element_blank())
# Arrange plots in 2x3 grid
panel_plot <- grid.arrange(p1, p3, p5,
p2, p4, p6,
nrow = 2, ncol = 3)
# Save plot
# Save plot
ggsave("time_series_panel.png", panel_plot, width = 15, height = 10)
# Save plot
ggsave("C:/Users/sfolk/Downloads/time_series_panel.png", panel_plot, width = 15, height = 10)
library(timetk)
train <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/train.csv")
library(modeltime)
install.packages("timetk")
install.packages("modeltime")
library(timetk)
library(modeltime)
cv_split <- time_series_split(train, assess="3 months", cumulative = TRUE)
library(tidymodels)
library(embed)
library(ggplot2)
library(recipes)
library(vroom)
library(themis)
library(discrim)
library(keras)
library(tensorflow)
library(tune)
library(dials)
library(workflows)
library(gridExtra)
library(forecast)
library(timetk)
library(vroom)
library(modeltime)
train <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/train.csv")
test <- vroom("C:/Users/sfolk/Desktop/STAT348/Demand/test.csv")
storeItem <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
cv_split <- time_series_split(train, assess="3 months", cumulative = TRUE)
cv_split <- time_series_split(train, assess="3 months", cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative = TRUE)
storeItemTrain <- train |>
filter(store == 4, item == 10)
cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
# Prepare the data for two different store-item combinations
storeItem1 <- train |>
filter(store == 4, item == 10)
storeItem2 <- train |>
filter(store == 3, item == 5)
# Split data for each store-item combination
cv_split1 <- time_series_split(storeItem1,
assess="3 months",
cumulative = TRUE)
cv_split2 <- time_series_split(storeItem2,
assess="3 months",
cumulative = TRUE)
# Create recipe for each dataset
arima_recipe1 <- recipe(sales ~ date, data = training(cv_split1)) %>%
step_timeseries_signature(date) %>%
step_rm(date) %>%
step_normalize(contains("index.num"),
contains("year"),
contains("month"),
contains("week"),
contains("day"))
arima_recipe2 <- recipe(sales ~ date, data = training(cv_split2)) %>%
step_timeseries_signature(date) %>%
step_rm(date) %>%
step_normalize(contains("index.num"),
contains("year"),
contains("month"),
contains("week"),
contains("day"))
# Define ARIMA model
arima_model <- arima_reg(seasonal_period=12,
non_seasonal_ar=5,
non_seasonal_ma=5,
seasonal_ar=2,
seasonal_ma=2,
non_seasonal_differences=2,
seasonal_differences=2
) %>%
set_engine("auto_arima")
# Create and fit workflows
arima_wf1 <- workflow() %>%
add_recipe(arima_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
# Create recipe for each dataset
arima_recipe1 <- recipe(sales ~ date, data = training(cv_split1)) %>%
step_timeseries_signature(date) %>%
step_rm(date, contains("lbl")) %>%  # Remove label columns
step_normalize(contains("index.num"),
contains("year"),
contains("month.abb"),  # Use numeric month representations
contains("week"),
contains("day"))
arima_recipe2 <- recipe(sales ~ date, data = training(cv_split2)) %>%
step_timeseries_signature(date) %>%
step_rm(date, contains("lbl")) %>%  # Remove label columns
step_normalize(contains("index.num"),
contains("year"),
contains("month.abb"),  # Use numeric month representations
contains("week"),
contains("day"))
# Define ARIMA model
arima_model <- arima_reg(seasonal_period=12,
non_seasonal_ar=5,
non_seasonal_ma=5,
seasonal_ar=2,
seasonal_ma=2,
non_seasonal_differences=2,
seasonal_differences=2
) %>%
set_engine("auto_arima")
# Create and fit workflows
arima_wf1 <- workflow() %>%
add_recipe(arima_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
# Create and fit workflows
arima_wf1 <- workflow() %>%
add_recipe(arima_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf2 <- workflow() %>%
add_recipe(arima_recipe2) %>%
add_model(arima_model) %>%
fit(data=training(cv_split2))
# Create recipe for each dataset
arima_recipe1 <- recipe(sales ~ date, data = training(cv_split1)) %>%
step_timeseries_signature(date) %>%
step_rm(contains("lbl")) %>%  # Remove label columns
step_normalize(contains("index.num"),
contains("year"),
contains("month"),
contains("week"),
contains("day"))
arima_recipe2 <- recipe(sales ~ date, data = training(cv_split2)) %>%
step_timeseries_signature(date) %>%
step_rm(contains("lbl")) %>%  # Remove label columns
step_normalize(contains("index.num"),
contains("year"),
contains("month"),
contains("week"),
contains("day"))
# Define ARIMA model
arima_model <- arima_reg(seasonal_period=12,
non_seasonal_ar=5,
non_seasonal_ma=5,
seasonal_ar=2,
seasonal_ma=2,
non_seasonal_differences=2,
seasonal_differences=2
) %>%
set_engine("auto_arima")
# Create and fit workflows
arima_wf1 <- workflow() %>%
add_recipe(arima_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
arima_wf2 <- workflow() %>%
add_recipe(arima_recipe2) %>%
add_model(arima_model) %>%
fit(data=training(cv_split2))
# Calibrate models
cv_results1 <- modeltime_calibrate(arima_wf1,
new_data = testing(cv_split1))
# Create and fit workflows
arima_wf1 <- workflow() %>%
add_recipe(arima_recipe1) %>%
add_model(arima_model) %>%
fit(data=training(cv_split1))
